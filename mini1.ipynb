{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython.display import Image\n",
    "import gym \n",
    "import itertools \n",
    "import matplotlib \n",
    "import matplotlib.style \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys \n",
    "from collections import defaultdict \n",
    "import plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([inf, inf, inf, inf, inf, inf, inf, inf], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00480614  1.408443    0.4867928  -0.11010541 -0.00556231 -0.11026593\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.00953903,  1.4053781 ,  0.47693357, -0.13623327, -0.00916664,\n",
      "       -0.07209262,  0.        ,  0.        ], dtype=float32), 0.22167695872278273, False, {})\n"
     ]
    }
   ],
   "source": [
    "print(env.step(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0, \n",
    "                            alpha = 0.6, epsilon = 0.1): \n",
    "    \"\"\" \n",
    "    Q-Learning algorithm: Off-policy TD control. \n",
    "    Finds the optimal greedy policy while improving \n",
    "    following an epsilon-greedy policy\"\"\"\n",
    "       \n",
    "    # Action value function \n",
    "    # A nested dictionary that maps \n",
    "    # state -> (action -> action-value). \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) \n",
    "   \n",
    "    # Keeps track of useful statistics \n",
    "    stats = {}\n",
    "    stats[\"episode_lengths\"] = np.zeros(num_episodes)\n",
    "    stats[\"episode_rewards\"] = np.zeros(num_episodes)    \n",
    "       \n",
    "    # Create an epsilon greedy policy function \n",
    "    # appropriately for environment action space \n",
    "    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n) \n",
    "       \n",
    "    # For every episode \n",
    "    for ith_episode in range(num_episodes): \n",
    "           \n",
    "        # Reset the environment and pick the first action \n",
    "        state = env.reset().tobytes() \n",
    "           \n",
    "        for t in itertools.count(): \n",
    "               \n",
    "            # get probabilities of all actions from current state \n",
    "            action_probabilities = policy(state) \n",
    "   \n",
    "            # choose action according to  \n",
    "            # the probability distribution \n",
    "            action = np.random.choice(np.arange( \n",
    "                      len(action_probabilities)), \n",
    "                       p = action_probabilities) \n",
    "   \n",
    "            # take action and get reward, transit to next state \n",
    "            next_state, reward, done, _ = env.step(action) \n",
    "            next_state = next_state.tobytes()\n",
    "            # Update statistics \n",
    "            stats[\"episode_rewards\"][ith_episode] += reward \n",
    "            stats[\"episode_lengths\"][ith_episode] = t \n",
    "               \n",
    "            # TD Update \n",
    "            best_next_action = np.argmax(Q[next_state])     \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action] \n",
    "            td_delta = td_target - Q[state][action] \n",
    "            Q[state][action] += alpha * td_delta \n",
    "   \n",
    "            # done is True if episode terminated    \n",
    "            if done: \n",
    "                break\n",
    "                   \n",
    "            state = next_state \n",
    "       \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "    \"\"\" \n",
    "    Creates an epsilon-greedy policy based \n",
    "    on a given Q-function and epsilon. \n",
    "       \n",
    "    Returns a function that takes the state \n",
    "    as an input and returns the probabilities \n",
    "    for each action in the form of a numpy array  \n",
    "    of length of the action space(set of possible actions). \n",
    "    \"\"\"\n",
    "    def policyFunction(state): \n",
    "   \n",
    "        Action_probabilities = np.ones(num_actions, \n",
    "                dtype = float) * epsilon / num_actions \n",
    "                  \n",
    "        best_action = np.argmax(Q[state]) \n",
    "        Action_probabilities[best_action] += (1.0 - epsilon) \n",
    "        return Action_probabilities \n",
    "   \n",
    "    return policyFunction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, stats = qLearning(env, 10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/q-learning-in-python/z\n",
    "\n",
    "https://github.com/ikyath/ReinforcementLearning-MountainCar/blob/master/RL_MountainCar/Mountain_Car_RL.ipynb\n",
    "\n",
    "https://github.com/openai/gym/wiki/Table-of-environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stats[\"episode_rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
